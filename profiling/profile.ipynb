{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330dc8cc",
   "metadata": {},
   "source": [
    "# Profiling tiling of titiler-pgstac and titiler-xarray\n",
    "\n",
    "This notebook profiles code for tiling CMIP6 data via 2 methods:\n",
    "\n",
    "1. pgSTAC + COGs: The first method uses a (local or remote) pgSTAC database for storing metadata about COGs on S3. The libraries used are pgstac for reading STAC metadata and rio_tiler's rasterio for reading COGs on S3.\n",
    "2. kerchunk + netCDF: The second method uses a (local or S3) kerchunk reference file for NetCDF files stored on S3. The libraries used are xarray for reading the Zarr metadata and rio_tiler's XarrayReader for reading data from the NetCDFs on S3.\n",
    "3. Zarr stores with different chunking configurations.\n",
    "\n",
    "In the future, the following improvements and additions to this profiling code will be made:\n",
    "\n",
    "1. Test zarr stores with pyramids\n",
    "2. Test different tiles + sum the results.\n",
    "3. Test a higher resolution dataset.\n",
    "\n",
    "# Findings: Executive summary\n",
    "\n",
    "1. pgSTAC + COGs is fastest, assuming GDAL environment variables are configured optimally\n",
    "2. tiling using Zarr really depends on the chunking structure. Make sure that:\n",
    "   1. if optimizing for spatial visualization and not time series, only store 1 time step per chunk\n",
    "   2. Make sure the coordinates themselves are not chunked, otherwise this will require a lot of data fetches when loading the dataset\n",
    "3. Pyramids can be used to generate low resolution versions of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c1361-1e28-42a6-92b5-7c4bac4d9211",
   "metadata": {},
   "source": [
    "## Setup / Step 0\n",
    "\n",
    "1. Load some basic libraries\n",
    "2. Set an initial tile to test\n",
    "3. Add some AWS credentials\n",
    "4. Install any missing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cbd0f3-f88e-4c4e-b809-eecda8f5c14f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught exception: An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 34.213.188.203/32, TCP, from port: 5432, to port: 5432, ALLOW\" already exists\n",
      "Connected to database\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "import warnings\n",
    "import cmip6_zarr.eodc_hub_role\n",
    "from matplotlib.pyplot import imshow\n",
    "import morecantile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from profiler.main import Timer, cprofile_list_to_dict\n",
    "\n",
    "from cmip6_zarr import eodc_hub_role\n",
    "credentials = eodc_hub_role.fetch_and_set_credentials()\n",
    "import cmip6_pgstac.profile_pgstac as profile_pgstac\n",
    "pool = profile_pgstac.connect_to_database(credentials)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc702b3-f418-4281-a8e2-2fe0358d4e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install morecantile==3.4.0 loguru titiler titiler-pgstac\n",
    "#!pip install psycopg psycopg_binary psycopg_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f855b1-e765-41a4-9da9-fec2e0ec106f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2f56bf-04d3-46d8-86e1-13c3e0fbff25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 0, 1),\n",
       " (1, 1, 2),\n",
       " (2, 3, 3),\n",
       " (4, 6, 4),\n",
       " (9, 12, 5),\n",
       " (18, 24, 6),\n",
       " (37, 48, 7),\n",
       " (75, 96, 8),\n",
       " (150, 192, 9),\n",
       " (301, 384, 10),\n",
       " (603, 769, 11)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tms = morecantile.tms.get(\"WebMercatorQuad\")\n",
    "zooms = range(12) # Zoom 10 is the level at which you can see large roads, 15 is buildings\n",
    "xyz_tiles = []\n",
    "for z in zooms:\n",
    "    # lat/lon over central park\n",
    "    tile = tms.tile(-73.97, 40.78, z)\n",
    "    xyz_tiles.append((tile.x, tile.y, tile.z))\n",
    "\n",
    "xyz_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f1561b-185d-47dc-9b32-ca2d80f84e25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "temporal_resolution = \"daily\"\n",
    "model = \"GISS-E2-1-G\"\n",
    "variable = \"tas\"\n",
    "anon=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a131851",
   "metadata": {},
   "source": [
    "## Profile titiler-pgstac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec854450-c3d7-4cce-9379-13384fcc1b7b",
   "metadata": {},
   "source": [
    "To achieve the best performance, we set some GDAL environment variables.\n",
    "\n",
    "These variables are documented here https://developmentseed.org/titiler/advanced/performance_tuning/, but that advice is copied into comments below for ease of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68399603-1a76-4cb6-a1fb-f00c35f41c3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdal_env_vars = {\n",
    "    # By default GDAL reads the first 16KB of the file, then if that doesn't contain the entire metadata\n",
    "    # it makes one more request for the rest of the metadata.\n",
    "    # In environments where latency is relatively high, AWS S3,\n",
    "    # it may be beneficial to increase this value depending on the data you expect to read.    \n",
    "    'GDAL_INGESTED_BYTES_AT_OPEN': '32768',\n",
    "    # It's much better to set EMPTY_DIR because this prevents GDAL from making a LIST request.\n",
    "    # LIST requests are made for sidecar files, which does not apply for COGs    \n",
    "    'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',\n",
    "    # Tells GDAL to merge consecutive range GET requests.    \n",
    "    'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'YES',\n",
    "    # When set to YES, this attempts to download multiple range requests in parallel, reusing the same TCP connection. \n",
    "    # Note this is only possible when the server supports HTTP2, which many servers don't yet support.\n",
    "    # There's no downside to setting YES here.    \n",
    "    'GDAL_HTTP_MULTIPLEX': 'YES',\n",
    "    'GDAL_HTTP_VERSION': '2',\n",
    "    # Setting this to TRUE enables GDAL to use an internal caching mechanism. It's recommended (strongly).    \n",
    "    'VSI_CACHE': 'TRUE'\n",
    "}\n",
    "\n",
    "def set_or_unset_gdal(set_vars=True):\n",
    "    if set_vars:\n",
    "        for key, value in gdal_env_vars.items():\n",
    "            os.environ[key] = value\n",
    "    else:\n",
    "        for key, value in gdal_env_vars.items():\n",
    "            os.environ.pop(key, None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b8d9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if temporal_resolution == 'daily':\n",
    "    collection = f\"CMIP6_daily_{model}_{variable}\"\n",
    "elif temporal_resolution == 'monthly':\n",
    "    collection = f\"CMIP6_ensemble_monthly_median_{variable}\"\n",
    "\n",
    "query = {\n",
    "  \"collections\": [ collection ],\n",
    "  \"filter\": {\n",
    "    \"op\": \"t_intersects\",\n",
    "    \"args\": [\n",
    "      {\n",
    "        \"property\": \"datetime\"\n",
    "      },\n",
    "      {\n",
    "        \"interval\": [\n",
    "          \"1950-04-01T00:00:00Z\"           \n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"filter-lang\": \"cql2-json\"\n",
    "}\n",
    "\n",
    "gdal_results_df = {}\n",
    "niters = 1\n",
    "xyz_tile = (0,0,0)\n",
    "for settings in ['with_gdal_vars', 'without_gdal_vars']:\n",
    "    gdal_results_df[settings] = { 'tile times': [], 'mean total time': None }\n",
    "    results_timings = gdal_results_df[settings]\n",
    "    if settings == 'with_gdal_vars':\n",
    "        set_or_unset_gdal(set_vars=True)\n",
    "    else:\n",
    "        set_or_unset_gdal(set_vars=False)\n",
    "    for iter in range(niters):\n",
    "        with Timer() as t:\n",
    "            image_and_assets, cprofile = profile_pgstac.tile(pool, *xyz_tile, query=dict(query))\n",
    "        results_timings['tile times'].append(round(t.elapsed * 1000, 2))\n",
    "    results_timings['mean total time'] = np.mean(results_timings['tile times'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488547ee-bf92-4a50-be0f-4a7139a722e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(cog_results_df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4b287-4d6f-405d-a5eb-5e2ab82c4564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Results for different tiles\n",
    "tile_results_df = {}\n",
    "niters = 10\n",
    "set_or_unset_gdal(set_vars=True)\n",
    "for xyz_tile in xyz_tiles:\n",
    "    tile_results_df[xyz_tile] = { 'tile times': [], 'mean total time': None }\n",
    "    results_timings = tile_results_df[xyz_tile]\n",
    "    for iter in range(niters):\n",
    "        with Timer() as t:\n",
    "            image_and_assets, cprofile = profile_pgstac.tile(pool, *xyz_tile, query=dict(query))\n",
    "        results_timings['tile times'].append(round(t.elapsed * 1000, 2))\n",
    "    results_timings['mean total time'] = np.mean(results_timings['tile times'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b59035-2639-45cd-8b2c-532d4f249d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(tile_results_df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1855e96-3d84-4ae3-a677-10fffb002dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = image_and_assets[0].data_as_image()\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021ba14",
   "metadata": {},
   "source": [
    "# Profile titiler-xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d60aad91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# useful to always reload the module while its being developed\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import xarray_tile_reader\n",
    "import zarr_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64be1ae-02b4-445c-842b-ea033d8ef53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = 'nasa-eodc-data-store'\n",
    "chunk_set_paths = ['600_1440_1', '600_1440_29', '365_262_262']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddd008-8ba4-48d0-8b01-a2b38964a906",
   "metadata": {},
   "source": [
    "# Create a table with the different chunk shapes and sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d48b74db-4f6c-4265-8135-0b1d76e431b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group is None\n",
      "Group is None\n",
      "Group is None\n",
      "Group is None\n"
     ]
    }
   ],
   "source": [
    "datastores = { \n",
    "    'kerchunk': {\n",
    "        \"data_store_path\": f\"combined_CMIP6_{temporal_resolution}_{model}_{variable}_kerchunk.json\"\n",
    "    },\n",
    "    'pyramid': {\n",
    "        \"data_store_path\": f\"pyramid/CMIP6_{temporal_resolution}_{model}_{variable}.zarr\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk_set_path in chunk_set_paths:\n",
    "    datastores[chunk_set_path] = {\n",
    "        'data_store_path': f\"{chunk_set_path}/CMIP6_{temporal_resolution}_{model}_{variable}.zarr\"\n",
    "    }\n",
    "\n",
    "for key, datastore in datastores.items():\n",
    "    reference = False\n",
    "    if key == 'kerchunk':\n",
    "        reference = True\n",
    "    if key == 'pyramid':\n",
    "        continue # skip for now\n",
    "    data_store_url = f\"s3://{bucket}/{datastore['data_store_path']}\"\n",
    "\n",
    "    ds = zarr_reader.xarray_open_dataset(data_store_url, anon=False, reference=reference)\n",
    "\n",
    "    var = ds[0][variable]\n",
    "    chunks = var.encoding.get(\"chunks\", \"N/A\")\n",
    "    dtype = var.encoding.get(\"dtype\", \"N/A\")\n",
    "    chunks_dict = dict(zip(var.dims, chunks))\n",
    "    chunk_size_mb = \"N/A\" if chunks is None else (np.prod(chunks) * dtype.itemsize)/1024/1024    \n",
    "    datastores[key]['chunk_size_mb'] = chunk_size_mb    \n",
    "    datastores[key]['chunks'] = chunks_dict\n",
    "    datastores[key]['dtype'] = dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a2da8c-a345-4de6-bcc8-f1757f4a0983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_store_path</th>\n",
       "      <th>chunk_size_mb</th>\n",
       "      <th>chunks</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kerchunk</th>\n",
       "      <td>combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk....</td>\n",
       "      <td>3.295898</td>\n",
       "      <td>{'time': 1, 'lat': 600, 'lon': 1440}</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pyramid</th>\n",
       "      <td>pyramid//CMIP6_daily_GISS-E2-1-G_tas.zarr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600_1440_1</th>\n",
       "      <td>600_1440_1/CMIP6_daily_GISS-E2-1-G_tas.zarr</td>\n",
       "      <td>3.295898</td>\n",
       "      <td>{'time': 1, 'lat': 600, 'lon': 1440}</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600_1440_29</th>\n",
       "      <td>600_1440_29/CMIP6_daily_GISS-E2-1-G_tas.zarr</td>\n",
       "      <td>95.581055</td>\n",
       "      <td>{'time': 29, 'lat': 600, 'lon': 1440}</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365_262_262</th>\n",
       "      <td>365_262_262/CMIP6_daily_GISS-E2-1-G_tas.zarr</td>\n",
       "      <td>95.577469</td>\n",
       "      <td>{'time': 365, 'lat': 262, 'lon': 262}</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data_store_path  chunk_size_mb  \\\n",
       "kerchunk     combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk....       3.295898   \n",
       "pyramid              pyramid//CMIP6_daily_GISS-E2-1-G_tas.zarr            NaN   \n",
       "600_1440_1         600_1440_1/CMIP6_daily_GISS-E2-1-G_tas.zarr       3.295898   \n",
       "600_1440_29       600_1440_29/CMIP6_daily_GISS-E2-1-G_tas.zarr      95.581055   \n",
       "365_262_262       365_262_262/CMIP6_daily_GISS-E2-1-G_tas.zarr      95.577469   \n",
       "\n",
       "                                            chunks    dtype  \n",
       "kerchunk      {'time': 1, 'lat': 600, 'lon': 1440}  float32  \n",
       "pyramid                                        NaN      NaN  \n",
       "600_1440_1    {'time': 1, 'lat': 600, 'lon': 1440}  float32  \n",
       "600_1440_29  {'time': 29, 'lat': 600, 'lon': 1440}  float32  \n",
       "365_262_262  {'time': 365, 'lat': 262, 'lon': 262}  float32  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(datastores, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28234cfd-8b77-47a6-9899-eff51ec40086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://nasa-eodc-data-store/combined_CMIP6_daily_GISS-E2-1-G_tas_kerchunk.json\n",
      "{'reference': True, 'anon': False, 'group': None}\n",
      "Group is None\n",
      "> \u001b[0;32m/home/jovyan/tile-benchmarking/profiling/zarr_reader.py\u001b[0m(42)\u001b[0;36mxarray_open_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     40 \u001b[0;31m        \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mxr_open_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m    \u001b[0mtime_to_open\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melapsed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_to_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/jovyan/tile-benchmarking/profiling/zarr_reader.py\u001b[0m(61)\u001b[0;36mget_variable\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     59 \u001b[0;31m        \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdim_to_drop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdim_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     60 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 61 \u001b[0;31m    \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     62 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     63 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://nasa-eodc-data-store/pyramid/CMIP6_daily_GISS-E2-1-G_tas.zarr\n",
      "{'reference': False, 'anon': False, 'group': 0}\n",
      "Group is 0\n",
      "> \u001b[0;32m/home/jovyan/tile-benchmarking/profiling/zarr_reader.py\u001b[0m(42)\u001b[0;36mxarray_open_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     40 \u001b[0;31m        \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mxr_open_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m    \u001b[0mtime_to_open\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melapsed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_to_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    *empty*\n",
      "Attributes:\n",
      "    multiscales:  [{'datasets': [{'path': '0', 'pixels_per_tile': 128}, {'pat...\n",
      "    title:        multiscale data pyramid\n",
      "    version:      0.0.6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  xr_open_args\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decode_coords': 'all', 'decode_times': False, 'consolidated': True}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "niters = 1\n",
    "tile_results_df = {}\n",
    "dataset_chunk_sizes = {}\n",
    "results_df = datastores.copy()\n",
    "\n",
    "for xyz_tile in xyz_tiles:\n",
    "    results_df[xyz_tile] = {}\n",
    "    for dataset in datastores.keys():\n",
    "        data_store_path = datastores[dataset]['data_store_path']\n",
    "        data_store_url = f\"s3://{bucket}/{data_store_path}\"\n",
    "        print(data_store_url)\n",
    "        reference, group = False, None\n",
    "        if dataset == 'kerchunk':\n",
    "            reference = True\n",
    "        if dataset == 'pyramid':\n",
    "            group = xyz_tile[2]\n",
    "\n",
    "        results_df[xyz_tile][dataset] = {\n",
    "            'time to open (ms)': [],\n",
    "            'rio reproject (ms)': [],\n",
    "            'total time (ms)': []\n",
    "        }\n",
    "        timings_results = results_df[xyz_tile][dataset]\n",
    "        for iter in range(niters):\n",
    "            with Timer() as t:\n",
    "                image_and_timings, cprofile = xarray_tile_reader.tile(\n",
    "                    data_store_url,\n",
    "                    *xyz_tile,\n",
    "                    reference=reference,\n",
    "                    anon=False,\n",
    "                    variable=variable,\n",
    "                    group = group\n",
    "                )\n",
    "            total_time = round(t.elapsed * 1000, 2)\n",
    "\n",
    "            timings = image_and_timings[1]\n",
    "            timings_results['time to open (ms)'].append(timings['time_to_open']),\n",
    "            timings_results['rio reproject (ms)'].append(timings['rio.reproject']),\n",
    "            timings_results['total time (ms)'].append(total_time)\n",
    "        timings_results['mean time to open (ms)'] = np.mean(timings_results['time to open (ms)'])\n",
    "        timings_results['mean rio reproject (ms)'] = np.mean(timings_results['rio reproject (ms)']) \n",
    "        timings_results['mean total time (ms)'] = np.mean(timings_results['total time (ms)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367c346-bb2f-4a1c-a4f1-876d402d078c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for xyz_tile in xyz_tiles:\n",
    "    print(f\"Results for {xyz_tile}\")\n",
    "    df = pd.DataFrame.from_dict(results_df[xyz_tile], orient='index')\n",
    "    print(df.sort_values('mean total time (ms)'))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
