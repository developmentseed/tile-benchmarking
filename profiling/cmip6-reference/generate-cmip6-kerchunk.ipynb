{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7250a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"temporal_resolution\",\n",
    "    choices=[\"daily\", \"monthly\"],\n",
    "    help=\"Specify the CMIP collection to use (daily or monthly)\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"local_or_remote\",\n",
    "    default=\"local\",\n",
    "    choices=[\"local\", \"remote\"],\n",
    "    help=\"Specify if the kerchunk file should be stored on S3.\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a201d8d",
   "metadata": {},
   "source": [
    "TODO(aimee): Make creating the kerchunk reference optional,\n",
    "since kerchunk files may have already been created and we just want to upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e703eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if args.temporal_resolution == \"daily\":\n",
    "    print(\"Running kerchunk generation for daily CMIP6 data...\")\n",
    "    temporal_resolution = \"daily\"\n",
    "    anon = True\n",
    "    s3_path = \"s3://nex-gddp-cmip6/NEX-GDDP-CMIP6/GISS-E2-1-G/historical/r1i1p1f2/tas/\"\n",
    "    # Your code for daily frequency goes here\n",
    "elif args.temporal_resolution == \"monthly\":\n",
    "    print(\"Running kerchunk generation for monthly CMIP6 data...\")\n",
    "    temporal_resolution = \"monthly\"\n",
    "    anon = False\n",
    "    s3_path = \"s3://climatedashboard-data/cmip6/raw/monthly/CMIP6_ensemble_median/tas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7879ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=anon, skip_instance_cache=False)\n",
    "fs_write = fsspec.filesystem(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a799e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve list of available months\n",
    "files_paths = fs_read.glob(s3_path)\n",
    "print(f\"{len(files_paths)} discovered from {s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we prepend the prefix 's3://', which points to AWS.\n",
    "if temporal_resolution == \"monthly\":\n",
    "    subset_files = sorted([\"s3://\" + f for f in files_paths if ('month_ensemble-median' in f and (\"1950\" in f or \"1951\" in f))])\n",
    "elif temporal_resolution == \"daily\":\n",
    "    subset_files = sorted([\"s3://\" + f for f in files_paths if \"1950.nc\" in f or \"1951.nc\" in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a11b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(subset_files)} file paths were retrieved.\")\n",
    "subset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02515b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = dict(mode=\"rb\", anon=anon, default_fill_cache=False, default_cache_type=\"first\")\n",
    "output_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d06adf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# We are creating a temporary directory to store the .json reference files\n",
    "# Alternately, you could write these to cloud storage.\n",
    "td = TemporaryDirectory()\n",
    "temp_dir = td.name\n",
    "print(f\"Writing single file references to {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Kerchunk's `SingleHdf5ToZarr` method to create a `Kerchunk` index from a NetCDF file.\n",
    "def generate_json_reference(u, temp_dir: str):\n",
    "    with fs_read.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        fname = u.split(\"/\")[-1].strip(\".nc\")\n",
    "        outf = f\"{fname}.json\"\n",
    "        with open(outf, \"wb\") as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode())\n",
    "        return outf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d29770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through filelist to generate Kerchunked files. Good use for `Dask`\n",
    "output_files = []\n",
    "for single_file in subset_files:\n",
    "    out_file = generate_json_reference(single_file, temp_dir)\n",
    "    output_files.append(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ec232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine individual references into single consolidated reference\n",
    "mzz = MultiZarrToZarr(\n",
    "    output_files,\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon': anon},\n",
    "    concat_dims=['time'],\n",
    "    coo_map={\"time\": \"cf:time\"},\n",
    "    inline_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f47dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_kerchunk = mzz.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write kerchunk .json record\n",
    "output_fname = f\"combined_{temporal_resolution}_cmip6_kerchunk.json\"\n",
    "with open(f\"{output_fname}\", \"wb\") as f:\n",
    "    print(f\"Writing combined kerchunk reference file {output_fname}\")\n",
    "    f.write(ujson.dumps(multi_kerchunk).encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset as zarr object using fsspec reference file system and Xarray\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\", fo=multi_kerchunk, remote_protocol=\"s3\", remote_options={\"anon\": anon}\n",
    ")\n",
    "m = fs.get_mapper(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "ds = xr.open_dataset(m, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'nasa-eodc-data-store'\n",
    "if args.local_or_remote == 'remote':\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.upload_file(output_fname, bucket_name, output_fname)\n",
    "    print(f\"Response uploading {output_fname} to {bucket_name} was {response}.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
