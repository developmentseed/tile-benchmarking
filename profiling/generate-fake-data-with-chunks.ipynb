{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b69a0d-6113-4cad-ac2b-9cb841ebebbd",
   "metadata": {},
   "source": [
    "# Generate fake data with the same chunk size\n",
    "\n",
    "In this notebook, we generate multiple data stores of increasingly fine resolution so that the total spatial size of the dataset grows by 2 on each iteration.\n",
    "\n",
    "This is so we can understand the relationship between the number of chunks and tiling performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3c7de-b19d-40bd-b87d-025e878b500e",
   "metadata": {},
   "source": [
    "## Setup 1: Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef6086a1-8af0-4afd-8929-e8b739e47749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import s3fs\n",
    "import sys; sys.path.append('..')\n",
    "import eodc_hub_role\n",
    "import zarr_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5b590-1c34-4511-9577-4baca3b14073",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup 2: Setup data storage\n",
    "\n",
    "Store data in the fake data directory in a \"with chunks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33c64fe9-f6ea-40fe-8576-94d4cb99b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = eodc_hub_role.fetch_and_set_credentials()\n",
    "bucket = 'nasa-eodc-data-store'\n",
    "fake_data_dir = 'fake_data'\n",
    "s3_fs = s3fs.S3FileSystem(\n",
    "    key=credentials['AccessKeyId'],\n",
    "    secret=credentials['SecretAccessKey'],\n",
    "    token=credentials['SessionToken'], \n",
    "    anon=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e277a-a6c9-499f-be53-f11ae9345eb7",
   "metadata": {},
   "source": [
    "## Fake Data Generation Part 1: Generate data stores with a single chunk\n",
    "\n",
    "These datastores will have varying chunk size since we generate them at varying resolution but no spatial chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c621a469-8734-4c6b-8947-4f7eb69389a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define starting conditions\n",
    "time_steps = 1\n",
    "ydim = 512\n",
    "xdim = 1024\n",
    "multiple = 2 # how much do you want the dataset to grow by each iteration\n",
    "n_multiples = 7\n",
    "data_path = 'fake_data/single_chunk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d2ab0f1-4281-490f-bf01-61a51d5627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, remove anything that is there\n",
    "#!aws s3 rm --recursive s3://{bucket}/{data_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b08ef607-221f-4b41-8244-e676a235b406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat512_lon1024.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat724_lon1448.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat1024_lon2048.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat1448_lon2896.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat2048_lon4096.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat2896_lon5792.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat4096_lon8192.zarr\n"
     ]
    }
   ],
   "source": [
    "# generate and store data\n",
    "zarr_helpers.generate_multiple_datastores(\n",
    "    n_multiples,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    f'{bucket}/{data_path}',\n",
    "    s3_fs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e86b6-ebde-46fe-8245-ca9a0e0d7d40",
   "metadata": {},
   "source": [
    "### Check that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c732b69e-856a-4415-b77d-325a55149c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat1024_lon2048.zarr: 16.0 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat1448_lon2896.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat2048_lon4096.zarr: 64.0 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat2896_lon5792.zarr: 127.97 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat4096_lon8192.zarr: 256.0 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat512_lon1024.zarr: 4.0 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/single_chunk/store_lat724_lon1448.zarr: 8.0 MB\n"
     ]
    }
   ],
   "source": [
    "directories = s3_fs.ls(f'{bucket}/{data_path}')\n",
    "for path in directories:\n",
    "    try:\n",
    "        # Attempt to open the Zarr store using xarray\n",
    "        store = s3fs.S3Map(root=path, s3=s3_fs, check=False)\n",
    "        ds = xr.open_zarr(store)\n",
    "        chunk_size = round(zarr_helpers.get_chunk_size(ds['data'])[2], 2)\n",
    "        print(f\"Chunk size for {path}: {chunk_size} MB\")\n",
    "    except Exception as e:\n",
    "        # Print an error message if unable to open the Zarr store\n",
    "        print(f\"Could not open {item} as a Zarr store. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e4be-73df-47fd-8597-d434fc23a624",
   "metadata": {},
   "source": [
    "## Fake Data Generation Part 2\n",
    "\n",
    "### Part 2 Step 1: Define starting conditions for generating data of the same chunk size, but varied chunk shape\n",
    "\n",
    "The following are set as variables so tests can be modified easily for different starting conditions. For example, we might want to test a different target chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb2474fc-32b9-400d-b217-7cc35f21cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each dataset will have chunks of the following dimensions {'time': 1, 'lat': 1448, 'lon': 2896}.\n",
      "We will generate 5 datasets, each being 2 times larger.\n"
     ]
    }
   ],
   "source": [
    "# Define starting conditions\n",
    "# variable: target size of chunks in mb\n",
    "target_size = 32\n",
    "# not variable: bytes per mb\n",
    "onemb = 1024 # bytes per mb\n",
    "# number of data values per chunk\n",
    "data_values_per_chunk = (target_size * onemb * onemb)/8 # 8 bytes for each data value\n",
    "# since there are half as many latitudes as longitudes, calculate the y dimension to be half the x dimension\n",
    "ydim = round(np.sqrt(data_values_per_chunk/2))\n",
    "xdim = 2*ydim\n",
    "target_chunks = {'time': 1, 'lat': ydim, 'lon': xdim}\n",
    "print(f\"Each dataset will have chunks of the following dimensions {target_chunks}.\")\n",
    "\n",
    "# timesteps are 1 for now\n",
    "time_steps = 1\n",
    "# how much do you want the dataset to grow by each iteration\n",
    "multiple = 2 \n",
    "# how many datasets we want to test\n",
    "n_multiples = 5\n",
    "print(f\"We will generate {n_multiples} datasets, each being {multiple} times larger.\")\n",
    "\n",
    "data_path = 'fake_data/with_chunks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a051070-26fe-4909-809d-a65883c98e35",
   "metadata": {},
   "source": [
    "### Part 2 Step 2: Generate Datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7760240c-068c-4321-86fb-94c76d728403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, remove anything that is there\n",
    "#!aws s3 rm --recursive s3://{bucket}/{data_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfa84169-cd99-4a8e-9b70-c41a5c5d545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat1448_lon2896.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat2048_lon4096.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat2896_lon5792.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat4096_lon8192.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat5793_lon11586.zarr\n"
     ]
    }
   ],
   "source": [
    "zarr_helpers.generate_multiple_datastores(\n",
    "    n_multiples,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    f'{bucket}/{data_path}',\n",
    "    s3_fs,\n",
    "    target_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde3f2a-e64d-462c-9257-2775b81c70a7",
   "metadata": {},
   "source": [
    "### Part 2 Step 3 (Optional): Check that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22c88659-3366-47d5-8732-66f325f27400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nasa-eodc-data-store/fake_data/with_chunks/store_lat1448_lon2896.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat2048_lon4096.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat2896_lon5792.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat4096_lon8192.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat5793_lon11586.zarr']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all items in the directory\n",
    "directories = s3_fs.ls(f'{bucket}/{data_path}')\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed7378a7-867c-4878-afe2-d1c040ecab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat1448_lon2896.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat2048_lon4096.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat2896_lon5792.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat4096_lon8192.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat5793_lon11586.zarr: 31.99 MB\n"
     ]
    }
   ],
   "source": [
    "for path in directories:\n",
    "    try:\n",
    "        # Attempt to open the Zarr store using xarray\n",
    "        store = s3fs.S3Map(root=path, s3=s3_fs, check=False)\n",
    "        ds = xr.open_zarr(store)\n",
    "        chunk_size = round(zarr_helpers.get_chunk_size(ds['data'])[2], 2)\n",
    "        print(f\"Chunk size for {path}: {chunk_size} MB\")\n",
    "    except Exception as e:\n",
    "        # Print an error message if unable to open the Zarr store\n",
    "        print(f\"Could not open {item} as a Zarr store. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-profiling",
   "language": "python",
   "name": "venv-profiling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
