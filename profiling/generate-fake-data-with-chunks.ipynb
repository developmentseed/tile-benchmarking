{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b69a0d-6113-4cad-ac2b-9cb841ebebbd",
   "metadata": {},
   "source": [
    "# Generate fake data with the same chunk size\n",
    "\n",
    "In this notebook, we generate multiple data stores of increasingly fine resolution so that the total spatial size of the dataset grows by 2 on each iteration.\n",
    "\n",
    "This is so we can understand the relationship between the number of chunks and tiling performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3c7de-b19d-40bd-b87d-025e878b500e",
   "metadata": {},
   "source": [
    "## Step 1: Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6086a1-8af0-4afd-8929-e8b739e47749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import s3fs\n",
    "import sys; sys.path.append('..')\n",
    "import eodc_hub_role\n",
    "import zarr_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5b590-1c34-4511-9577-4baca3b14073",
   "metadata": {},
   "source": [
    "## Step 2: Setup data storage\n",
    "\n",
    "Store data in the fake data directory in a \"with chunks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c64fe9-f6ea-40fe-8576-94d4cb99b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = eodc_hub_role.fetch_and_set_credentials()\n",
    "bucket = 'nasa-eodc-data-store'\n",
    "fake_data_dir = f'fake_data/with_chunks'\n",
    "s3_fs = s3fs.S3FileSystem(\n",
    "    key=credentials['AccessKeyId'],\n",
    "    secret=credentials['SecretAccessKey'],\n",
    "    token=credentials['SessionToken'], \n",
    "    anon=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e4be-73df-47fd-8597-d434fc23a624",
   "metadata": {},
   "source": [
    "## Step 3: Define starting starting conditions\n",
    "\n",
    "The following are set as variables so tests can be modified easily for different starting conditions. For example, we might want to test a different target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb2474fc-32b9-400d-b217-7cc35f21cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each dataset will have chunks of the following dimensions {'time': 1, 'lat': 1448, 'lon': 2896}.\n",
      "We will generate 5 datasets, each being 2 times larger.\n"
     ]
    }
   ],
   "source": [
    "# Define starting conditions\n",
    "# variable: target size of chunks in mb\n",
    "target_size = 32\n",
    "# not variable: bytes per mb\n",
    "onemb = 1024 # bytes per mb\n",
    "# number of data values per chunk\n",
    "data_values_per_chunk = (target_size * onemb * onemb)/8 # 8 bytes for each data value\n",
    "# since there are half as many latitudes as longitudes, calculate the y dimension to be half the x dimension\n",
    "y = round(np.sqrt(data_values_per_chunk/2))\n",
    "x = 2*y\n",
    "target_chunks = {'time': 1, 'lat': y, 'lon': x}\n",
    "print(f\"Each dataset will have chunks of the following dimensions {target_chunks}.\")\n",
    "\n",
    "# timesteps are 1 for now\n",
    "time_steps = 1\n",
    "\n",
    "# how much do you want the dataset to grow by each iteration\n",
    "multiple = 2 \n",
    "# how many datasets we want to test\n",
    "n_multiples = 5\n",
    "print(f\"We will generate {n_multiples} datasets, each being {multiple} times larger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a051070-26fe-4909-809d-a65883c98e35",
   "metadata": {},
   "source": [
    "## Step 4: Generate Datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7760240c-068c-4321-86fb-94c76d728403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, remove anything that is there\n",
    "# !aws s3 rm --recursive s3://{bucket}/{fake_data_dir}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a235fb8-7bc4-4a66-8d08-93fb1d8edf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_multiple in range(n_multiples):\n",
    "    # if this isn't the first iteration, grow the total size of the dataset by 2\n",
    "    if n_multiple != 0:\n",
    "        # expand grid by multiple\n",
    "        data_values_per_chunk = y * x * multiple\n",
    "        # to maintain the aspect ratio, where we know size == y * x and x = 2y\n",
    "        y = round(np.sqrt(data_values_per_chunk/2))\n",
    "        x = 2*y\n",
    "        print(f\"x is {x}, y is {y}\")\n",
    "        \n",
    "    data = np.random.random(size=(time_steps, y, x))\n",
    "\n",
    "    # Create Xarray datasets with dimensions and coordinates\n",
    "    ds = xr.Dataset({\n",
    "        'data': (['time', 'lat', 'lon'], data),\n",
    "    }, coords={\n",
    "        'time': np.arange(time_steps),\n",
    "        'lat': np.linspace(-90, 90, y),\n",
    "        'lon': np.linspace(-180, 180, x)\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        ds = ds.chunk(target_chunks)\n",
    "        path = f'{fake_data_dir}/store_lat_{y}x_lon_{x}.zarr'\n",
    "        store = s3fs.S3Map(root=f'{bucket}/{path}', s3=s3_fs, check=False)\n",
    "        ds.to_zarr(store, mode='w')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde3f2a-e64d-462c-9257-2775b81c70a7",
   "metadata": {},
   "source": [
    "## Step 5 (Optional): Check that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c88659-3366-47d5-8732-66f325f27400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nasa-eodc-data-store/fake_data/with_chunks/store_lat_1448x_lon_2896.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat_2048x_lon_4096.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat_2896x_lon_5792.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat_4096x_lon_8192.zarr',\n",
       " 'nasa-eodc-data-store/fake_data/with_chunks/store_lat_5793x_lon_11586.zarr']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all items in the directory\n",
    "directories = s3_fs.ls(f'{bucket}/{fake_data_dir}')\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed7378a7-867c-4878-afe2-d1c040ecab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat_1448x_lon_2896.zarr:\n",
      "31.9931640625\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat_2048x_lon_4096.zarr:\n",
      "31.9931640625\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat_2896x_lon_5792.zarr:\n",
      "31.9931640625\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat_4096x_lon_8192.zarr:\n",
      "31.9931640625\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk size for nasa-eodc-data-store/fake_data/with_chunks/store_lat_5793x_lon_11586.zarr:\n",
      "31.9931640625\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop through each item and open it with xarray if it's a Zarr store\n",
    "for path in directories:\n",
    "    # Check if the item is a directory (Zarr stores are directories)\n",
    "    try:\n",
    "        # Attempt to open the Zarr store using xarray\n",
    "        store = s3fs.S3Map(root=path, s3=s3_fs, check=False)\n",
    "        ds = xr.open_zarr(store)\n",
    "        print(f\"Chunk size for {path}:\")\n",
    "        print(zarr_helpers.get_chunk_size(ds['data'])[2])\n",
    "        print('-' * 80)  # Print a separator line\n",
    "    except Exception as e:\n",
    "        # Print an error message if unable to open the Zarr store\n",
    "        print(f\"Could not open {item} as a Zarr store. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dbc5c-f400-4d16-943e-3a8f206d2b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
