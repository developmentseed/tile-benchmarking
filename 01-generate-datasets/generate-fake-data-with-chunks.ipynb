{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b69a0d-6113-4cad-ac2b-9cb841ebebbd",
   "metadata": {},
   "source": [
    "# Generate fake data\n",
    "\n",
    "In this notebook, we generate multiple data stores of increasingly finer resolution so that the total spatial size of the dataset grows by 2 to allow for variation in chunk size and number of chunks.\n",
    "\n",
    "This is so we can understand the relationship between the size and number of chunks and tiling performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3c7de-b19d-40bd-b87d-025e878b500e",
   "metadata": {},
   "source": [
    "## Setup 1: Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68531d3-8911-4658-98d6-289ae78479a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install rio-tiler==4.1.11 loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6086a1-8af0-4afd-8929-e8b739e47749",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import s3fs\n",
    "import sys; sys.path.append('..')\n",
    "import helpers.eodc_hub_role as eodc_hub_role\n",
    "import zarr_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5b590-1c34-4511-9577-4baca3b14073",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup 2: Setup data storage\n",
    "\n",
    "Store data in the fake data directory in a \"with chunks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c64fe9-f6ea-40fe-8576-94d4cb99b337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = eodc_hub_role.fetch_and_set_credentials()\n",
    "bucket = 'nasa-eodc-data-store'\n",
    "fake_data_dir = 'test-data/fake-data'\n",
    "s3_fs = s3fs.S3FileSystem(\n",
    "    key=credentials['AccessKeyId'],\n",
    "    secret=credentials['SecretAccessKey'],\n",
    "    token=credentials['SessionToken'], \n",
    "    anon=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e277a-a6c9-499f-be53-f11ae9345eb7",
   "metadata": {},
   "source": [
    "## Fake Data Generation Part 1: Generate data stores with a single chunk\n",
    "\n",
    "These datastores will have varying chunk size since we generate them at varying resolution but no spatial chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c621a469-8734-4c6b-8947-4f7eb69389a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define starting conditions\n",
    "time_steps = 1\n",
    "ydim = 512\n",
    "xdim = 1024\n",
    "multiple = 2 # how much do you want the dataset to grow by each iteration\n",
    "n_multiples = 7\n",
    "data_path = 'single_chunk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d2ab0f1-4281-490f-bf01-61a51d5627c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are updating this data, remove anything that is there\n",
    "#!aws s3 rm --recursive s3://{bucket}/{data_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b08ef607-221f-4b41-8244-e676a235b406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat512_lon1024.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat724_lon1448.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat1024_lon2048.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat1448_lon2896.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat2048_lon4096.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat2896_lon5792.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/single_chunk/store_lat4096_lon8192.zarr\n"
     ]
    }
   ],
   "source": [
    "# generate and store data\n",
    "zarr_helpers.generate_multiple_datastores(\n",
    "    n_multiples,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    f'{bucket}/{fake_data_dir}/{data_path}',\n",
    "    s3_fs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e86b6-ebde-46fe-8245-ca9a0e0d7d40",
   "metadata": {},
   "source": [
    "### Check that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c732b69e-856a-4415-b77d-325a55149c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat1024_lon2048.zarr: 16.0 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat1448_lon2896.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat2048_lon4096.zarr: 64.0 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat2896_lon5792.zarr: 127.97 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat4096_lon8192.zarr: 256.0 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat512_lon1024.zarr: 4.0 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/single_chunk/store_lat724_lon1448.zarr: 8.0 MB\n"
     ]
    }
   ],
   "source": [
    "directories = s3_fs.ls(f'{bucket}/{fake_data_dir}/{data_path}')\n",
    "for path in directories:\n",
    "    try:\n",
    "        # Attempt to open the Zarr store using xarray\n",
    "        store = s3fs.S3Map(root=path, s3=s3_fs, check=False)\n",
    "        ds = xr.open_zarr(store)\n",
    "        chunk_size = round(zarr_helpers.get_chunk_size(ds['data'])[2], 2)\n",
    "        print(f\"Chunk size for {path}: {chunk_size} MB\")\n",
    "    except Exception as e:\n",
    "        # Print an error message if unable to open the Zarr store\n",
    "        print(f\"Could not open {item} as a Zarr store. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e4be-73df-47fd-8597-d434fc23a624",
   "metadata": {},
   "source": [
    "## Fake Data Generation Part 2\n",
    "\n",
    "### Part 2 Step 1: Define starting conditions for generating data of the same chunk size, but varied chunk shape\n",
    "\n",
    "The following are set as variables so tests can be modified easily for different starting conditions. For example, we might want to test a different target chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2474fc-32b9-400d-b217-7cc35f21cac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each dataset will have chunks of the following dimensions {'time': 1, 'lat': 1448, 'lon': 2896}.\n",
      "We will generate 5 datasets, each being 2 times larger.\n"
     ]
    }
   ],
   "source": [
    "# Define starting conditions\n",
    "# variable: target size of chunks in mb\n",
    "target_size = 32\n",
    "# not variable: bytes per mb\n",
    "onemb = 1024 # bytes per mb\n",
    "# number of data values per chunk\n",
    "data_values_per_chunk = (target_size * onemb * onemb)/8 # 8 bytes for each data value\n",
    "# since there are half as many latitudes as longitudes, calculate the y dimension to be half the x dimension\n",
    "ydim = round(np.sqrt(data_values_per_chunk/2))\n",
    "xdim = 2*ydim\n",
    "target_chunks = {'time': 1, 'lat': ydim, 'lon': xdim}\n",
    "print(f\"Each dataset will have chunks of the following dimensions {target_chunks}.\")\n",
    "\n",
    "# timesteps are 1 for now\n",
    "time_steps = 1\n",
    "# how much do you want the dataset to grow by each iteration\n",
    "multiple = 2 \n",
    "# how many datasets we want to test\n",
    "n_multiples = 5\n",
    "print(f\"We will generate {n_multiples} datasets, each being {multiple} times larger.\")\n",
    "\n",
    "data_path = 'with_chunks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a051070-26fe-4909-809d-a65883c98e35",
   "metadata": {},
   "source": [
    "### Part 2 Step 2: Generate Datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7760240c-068c-4321-86fb-94c76d728403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, remove anything that is there\n",
    "#!aws s3 rm --recursive s3://{bucket}/{data_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfa84169-cd99-4a8e-9b70-c41a5c5d545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat1448_lon2896.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat2048_lon4096.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat2896_lon5792.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat4096_lon8192.zarr\n",
      "Writing to nasa-eodc-data-store/fake_data/with_chunks/store_lat5793_lon11586.zarr\n"
     ]
    }
   ],
   "source": [
    "zarr_helpers.generate_multiple_datastores(\n",
    "    n_multiples,\n",
    "    xdim,\n",
    "    ydim,\n",
    "    f'{bucket}/{fake_data_dir}/{data_path}',\n",
    "    s3_fs,\n",
    "    target_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde3f2a-e64d-462c-9257-2775b81c70a7",
   "metadata": {},
   "source": [
    "### Part 2 Step 3 (Optional): Check that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c88659-3366-47d5-8732-66f325f27400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat1448_lon2896.zarr',\n",
       " 'nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat2048_lon4096.zarr',\n",
       " 'nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat2896_lon5792.zarr',\n",
       " 'nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat4096_lon8192.zarr',\n",
       " 'nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat5793_lon11586.zarr']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all items in the directory\n",
    "directories = s3_fs.ls(f'{bucket}/{fake_data_dir}/{data_path}')\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed7378a7-867c-4878-afe2-d1c040ecab60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat1448_lon2896.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat2048_lon4096.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat2896_lon5792.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat4096_lon8192.zarr: 31.99 MB\n",
      "Chunk size for nasa-eodc-data-store/test-data/fake-data/with_chunks/store_lat5793_lon11586.zarr: 31.99 MB\n"
     ]
    }
   ],
   "source": [
    "for path in directories:\n",
    "    try:\n",
    "        # Attempt to open the Zarr store using xarray\n",
    "        store = s3fs.S3Map(root=path, s3=s3_fs, check=False)\n",
    "        ds = xr.open_zarr(store)\n",
    "        chunk_size = round(zarr_helpers.get_chunk_size(ds['data'])[2], 2)\n",
    "        print(f\"Chunk size for {path}: {chunk_size} MB\")\n",
    "    except Exception as e:\n",
    "        # Print an error message if unable to open the Zarr store\n",
    "        print(f\"Could not open {item} as a Zarr store. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d7de1-55f7-461e-8a81-66a379bb9e09",
   "metadata": {},
   "source": [
    "## Capture datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae8bbb1-9dc2-4eee-a323-e67c5b426066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_paths = ['single_chunk', 'with_chunks']\n",
    "directories = s3_fs.ls(f'{bucket}/{fake_data_dir}/{data_paths[0]}')\n",
    "directories.extend(s3_fs.ls(f'{bucket}/{fake_data_dir}/{data_paths[1]}'))\n",
    "# Write output to json file\n",
    "datasets = {}\n",
    "variable = \"data\"\n",
    "for directory in directories:\n",
    "    dataset_id = '/'.join(directory.split('/')[-2:])\n",
    "    dataset_url = f\"s3://{bucket}/{directory}\"\n",
    "    datasets[dataset_id] = {\n",
    "        \"source\": dataset_url,\n",
    "        \"variable\": variable\n",
    "    }\n",
    "    \n",
    "with open(\"fake-datasets.json\", \"w\") as f:\n",
    "    f.write(json.dumps(datasets))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
